\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{color}

\newcommand{\MK}[1]{\textcolor{red}{MK: #1}}  % Manoj's comments

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Evaluation and Analysis of Methods for Facial Recognition Applied to Partially Occluded Images}

\author{\IEEEauthorblockN{Zahra Siddiqa}
\MK{School of Computing}\\
% * <manojpratnas@gmail.com> 2018-08-07T23:03:37.256Z:
%
% ^.
Dublin City University\\
Dublin, Ireland \\
\textit{zahara.hassan2@mail.dcu.ie
}}

\maketitle
\MK{my comments will be in red like this. You can remove them by removing the \\MK tag}

\begin{abstract}
The aim of face recognition is to give a system the capability to detect and identify human faces. Face recognition systems find applications in access and security, criminal identification, target advertising, computer vision and many other fields. It is easier to build a face recognition system when an image contains unobstructed view if the face. However, the probability of finding an image with a partial or occluded face is very high which hinders the performance of the system. This project aims to make a comparative study of existing methods of face recognition. Secondly, the project aims to study the conduct of these algorithms when they are applied on occluded images by randomly cropping out a certain area of an image. Based on this study, it can be verified if the existing methods can be utilized for face recognition of occluded images without having to compromise on the accuracy of the models. This project also confirms the higher degree of precision that can be achieved with the neural networks.

\end{abstract}

\begin{IEEEkeywords}
Face Recognition; Occlusion; Principal Component Analysis; k Nearest Neighbour; Convolutional Neural Networks;

\end{IEEEkeywords}

\section{Introduction}
\label{sec: 1.introduction}
Unlike face detection which deals with identifying the presence of a face in an image, face recognition deals with the feature extraction and recognition of the face in an image. Facial recognition techniques have proven beneficial predominantly in the field of security. Face recognition is the preferred choice over other biometrics as it is a non intrusive and robust method of identification. As a whole, face recognition can be seen as a classification task with inputs as images and the outputs as identity of people. 

\begin{figure}[h!]
%\vspace{-0.2cm}
 \centering
 \includegraphics[width = 10cm]{Face_Recognition_Architecture.JPG}
 \caption{ Face Recognition Architecture (Source:\cite{makwana_makwana_2016})}
 \label{fig 1: face recognition architecture}
\end{figure}

Face recognition is an extensively researched subject\cite{zhao2003face} \cite{abate20072d}.  Most of the face recognition systems today, perform under unobstructed conditions\cite{phillips2005overview}.In this systems, the faces in the images are directly pointed at the camera. However, in a real world scenario, images seldom contain faces which are unobstructed or  faces that are pointed directly at the camera. Occlusion is defined as the blockage due to an external factor. Occlusion in an image can be due to a number of factors like lighting conditions, glasses, scarves, different poses or even certain facial expressions. Designing a face recognition system that gives accurate results even with occlusion is a challenging task due to the missing facial cues from occluded areas of the face.

\subsection{Principal Component Analysis}
\label{subsec: 1a.Principal Component Analysis}
Principal Component Analysis: By definition, principal component analysis is a mathematical procedure that uses orthogonal transformation to convert a set of values of possibly correlated M face images into a set of K uncorrelated variables called the Eigenfaces.
\cite{turk1991face}.Face recognition by PCA is achieved by projecting the test image in the eigenface subspace. Then the distance between the position of test image in eigenspace is compared with the known dataset or the training dataset. Based on this comparison, the person is recognised. PCA gives excellent results in a constrained environment. It is a preferred approach as it is simple, fast and has a high learning capability.

\begin{figure}[h!]
%\vspace{-0.2cm}
 \centering
 \includegraphics[width = 9cm]{eigenface.JPG}
 \caption{ Most significant Eigenfaces}
 \label{fig 1: eigenface}
\end{figure}
	

\subsection{k-Nearest Neighbors } 
\label{subsec: 1b.Knn}
k-nearest neighbors is a robust and efficient method used for classification and regression. In this algorithm, the training image data set is separated into multiple classes to predict the classification of a new test image. Knn is one the besttt face recognition methods as it is a non parametric algorithm. This fact is very helpfulll as the data typically does not obey the theoretical assumptions. hence prior knowledge of the data is not required for prediction. the bsis of knn algorithm is feature-similarity  i.e. the closeness of the features of any test image to the training images determines the classification of that test image; where Eucledian distance measure is used to find the the distance in the image space \cite{wang2005euclidean}. The distance between test image features and training image feature is calculated and a distance matrix is formed. The distance matrix is summed up and arranged in an increasing order. Based on the value of k, the first k elements are selected and the test image is classified according to the majority class value. \cite{zhang2007ml}

\subsection{CNN}
\label{subsec: 1d.CNN}
Neural networks, is another widely preferred face recognition method because of its non linear approach. Convolutional Neural Networks based systems directly learn from the training dataset that is provided and adopt deep learning algorithms to recognise an image. The accuracy of these systems can be significantly improved by appropriate pre-processing and segmentation of the images and by increasing the number of hidden layers \cite{dean2012large}.
There are many libraries available to implement convolutional neural networks\cite{2016arXiv160502688full}\cite{Jia:2014:CCA:2647868.2654889}\cite{tensorflow2015-whitepaper}. TensorFlow is an open source library for machine learning developed by Google and is based on deep learning DistBelief framework. A convolutional neural network based on TensorFlow is highly robust and has a high accuracy rate even when the faces in the images are occluded.  It offers high flexibility and is relatively simple to use\cite{tensorflow2015-whitepaper}. An advantage of using convolutional neural networks is that some of the pre-processing steps can be skipped \cite{oliveira2017irish}. Keras is an open source library which runs on top of TensorFlow. Keras makes it easy to build and run convolutional neural networks\cite{chollet2015keras}. 

There has been a lot of research on face recognition systems under unoccluded image conditions. However there has not been enough research on face recognition under occlusion. In this project the methods that have been mentioned above are applied to occluded images. These methods have been chosen as they are state-of-the-art, robust and widely used in the industry. The goal of PCA is to retain only those features that convey the most information\cite{turk1991face}. k Nearest Neighbor algorithm, although is simple in implementation, provides reasonably accurate results\cite{zhang2007ml}.Convolutional neural networks are used for their superior performance and high accuracy\cite{lawrence1997face}.

\subsection{Data} 
\label{subsec: 1c.Data}
The dataset used for this project is the Labeled Faces in the Wild dataset, a collection of over 13,000 images of celebrities from the internet. Each face has been labelled and stored under a folder with the name of the person. However, not all people from the dataset have the same number of images. Using the dataset as is, will not yield expected results as any network will not be able to learn and identify a person based on just one image. In order to avoid class imbalance and for the system to train satisfactorily, only few people with most number of images have been selected for this project.(cite: http://vis-www.cs.umass.edu/lfw/). Occluded faces are more complicated to recognise than unoccluded ones. To get an occluded image dataset, the images from the training dataset have been randomly occludedddd. The human brain excels at recognising faces that could be significantly occluded. However most face recognising systems reduce to a chance level accuracy while identifying occluded images

\begin{figure}[h!]
%\vspace{-0.2cm}
 \centering
 \includegraphics[width = 9cm]{dataset.JPG}
  \caption{ Images from Labeled Faces in the Wild(Source:\cite{LFWTech})}
 \label{fig 1: dataset}
\end{figure}

\section{Background}
\label{sec: 2.Background}

There are three stages involved in the process of face recognition from an image. They are: (1) face detection (2) feature extraction (3) face recognition. Face detection, as mentioned earlier, is the technology to detect the presence of a human face in an image. Numerous methods can be used to perform face detection and One of the most popular method is to perform face detection is Viola Jones face detector \cite{zafeiriou2015survey}. Although it trains at a slow pace, the advantage of Viola Jones face detector over other methods is swift detection. It involves four key principles- the simple haar features, utilization of integral images and several rounds of boosting for feature extraction and selection, and an attentional cascade to only allow windows with faces\cite{zafeiriou2015survey}.

Feature extraction, in simple terms, deals with the removal of all the redundant information from an image, thereby increasing the computation speed and reducing the amount of memory used. Essentially, this stage includes all the pre-processing and normalization of the image. The steps involved are RGB to grayscale transformation, scaling and histogram equalization. Feature extraction is the most complex part of face recognition process. The accuracy of the system is dependent on the method used for feature extraction.

Face recognition methods have evolved over time and development of technology. A very simple technique to achieve this is to flatten the test image into a vector, and compute the Euclidean distance between the test image and all the other flattened images from the training dataset. The downside of this technique is that it consumes a large amount of time to compare all images if the dataset is large and is not very accurate. Face recognition methods can largely be classified into appearance-based and model based methods. Appearance based methods are applied to the whole face or particular areas of the face. On the other hand, model based methods are applied to shape and texture of the face. A model of the human face is constructed to detect the facial variations. Since appearance based methods do not use prior knowledge of faces, they fail when images of poor illumination or faces with different poses are tested. Conversely, model based approaches  make use of prior knowledge of faces and hence they are indifferent to any changes in the illumination and pose. However model based systems require a fair amount of time and are complex to implement. Appearance methods are further divided into linear and non linear methods. The most common among linear methods is the principal component analysis. An extension of principal component analysis to perform non linear form of face recognition is kernel principal component analysis. Another widely preferred non linear approach is the neural networks method \cite{agrawal2015evaluation}.

In the recent years, there has been some research into face recognition under partial occlusion.Blanz et al. introduced a face recognition method which recognises faces irrespective of the pose and light conditions \cite{blanz2003face}. In accordance with other model based approaches, they simulate a 3D image using graphics from single images. The model is then fitted to the images. Through this model, they account for the variations in pose and illumination. The model is trained from a set of 3D scans of the head. In this paper, they describe the design of the model and the algorithm for fitting it to the images. Finally they provide a framework for face recognition where the faces are represented by 3D shape and texture from the model. 

The methods used to handle occlusion can be classified as feature, part and fractal based methods. In part based methods, facial recognition is done by dividing the image into multiple parts. Tan et al. presented a non-metric partial similarity measure to calculate the outstanding similarities between two images and discarding all the unimportant differences between them \cite{tan2006learning}. This will help  extract the intra-personal similarities thereby causing the effect created by occlusions and other variations to be reduced. Kim et al. use two-dimensional principal component analysis for face recognition of occluded images \cite{kim2007occlusion}. Here, a feature matrix is created and the occluded parts of an image are identified by classifying each row of the matrix. The process is divided into two stages- occlusion detection and partial matching. They use a combination of k-NN and 1-NN classifier and similarities between feature matrices on AR face database to develop the face recognition algorithm. Chen et al. suggested the use of kernel principal component analysis algorithm \cite{chen2016recognition}. The images were divided and a certain weight was assigned to each part. The features from each part were then extracted by KPCA to classify and recognise the image.

In fractal based method for handling partial occlusions, the images are partitioned and each region is accessed by iterated function system. The ad hoc measurement of distance using a distance function removes all the unimportant data that is not required for recognition, thereby increasing the speed and accuracy of recognition \cite{abate20072d}.

Feature based methods that are applied to tackle the issue of occlusion take into account the features of an individual like the edges around the eyes and mouth and disregard the other features. Turk et al propose a face recognition method using eigenfaces. Images are represented as a small set of 2D characteristics \cite{turk1991face}. The eigenvectors of the training dataset define the  feature space. Images are projected onto a feature space to derive unique characteristics. Faces are recognised by comparing the characteristics of the test dataset to the characteristics of the test dataset. The model provided the  capability of near-real-time face recognition. A. M. Martinez takes a probabilistic approach to solve occlusion problem by using an eigenspace representation \cite{martinez2000recognition}. Each image is divided into multiple regions and is analysed to recreate the training set and perform a matching with the images to recognise the face.

Neural networks, is another widely preferred face recognition method because of its non linear approach. Convolutional Neural Networks based systems directly learn from the training dataset that is provided and adopt deep learning algorithms to recognise an image. Using an unaltered convolutional neural network on occluded images, a low level of accuracy will be obtained \cite{chandler2016mitigation}.However, training this network with occluded images improves the accuracy but is still not very efficient in prediction. This can be significantly improved by appropriate pre-processing and segmentation of the images. The flaw with this system is that excessive training will lead to the problem of overfit. Overfit is a situation where the system memorizes all the training data and when new test data with noise is presented, it will not be able to produce an accurate result.

At present, most of the research has been done on face detection and recognition systems of perfectly aligned face images and relatively few studies for images with facial occlusion. Of all the methods that are proposed, most researchers use part based methods in which the image is segmented and then each individual region is analyzed. The most widely implemented face recognition under occlusion techniques are face recognition using kernel principal component analysis, k-nearest neighbours and convolutional neural networks. Each of these methods have been considered on different image datasets. To measure the accuracy of these methods, they should be verified against one dataset. Hence here, all these methods have been implemented on Labelled Faces in the Wild dataset. Furthermore, there is a need to verify the behaviour of these models and find the change in accuracy when regions of the face are occluded in the image.





\section{Analysis of Process Migration in Containers}
\label{sec:3 Analysis Process Migration} 
Method
The following section describes a theoretical analysis of the methods followed to implement the three face recognition systems, i.e. principal component analysis, k nearest neighbour and convolutional neural networks:



 

\subsection{Principal Component Analysis}
\label{subsec:3.1 Principal Component Analysis}
Principal component analysis is a computationally meticulous approach for simplification of data. It aims to convert a set of correlated data into an uncorrelated set called principal components\cite{turk1991face}.Data is split into training and test data using cross validation technique.as the number of features increase, the computation cost and complexity of the system increase. it could lead to the problem of overfitting. Instead of indiscriminate elimination of image pixels,  Principal Component Analysis is employed to perform dimensionality reduction which can reduce the data from a two dimension plane to a single dimension line. PCA attempts to represent the variance in the training data with a reduced number of dimensions.  the features are compressed and only the most useful feature combinations are returned. these constitute the eigenfaces. the data is then normalized, to make it less redundant. it is performed by rotating into the coordinate space of the principal components. Each dimension is then divided by the standard deviation in that direction to get unit variance and then rotated back. 

Support Vector Machines and GridSearchCV have been used to train the model. Support Vector Machines determine a decision boundary such that the distance between two classes of data is maximum. the classifier can easily recognize faces when the data is spread out instead of grouped together. Support Vector Machines are very efficient. however, to further optimize the results the parameters can be altered manually or can be automated using GridSearch module that does a parameter sweep to give the best results.
The general steps to perform face recognition using PCA are:

	1. Train the system using a certain image set.
    
    
	2. Compute the covariance matrix of the training dataset
    
	3. Extract Eigenfaces using PCA
	
    4. Represent each image with respect to eigenfaces(as a vector of weights)

5. To test the system, calculate the weight of test image. Compare it with the weights already known to the system

6. Recognition is done by matching the weights of the test image to the closest weights that are known to the system. 

This model is then tested on an unknown dataset to measure accuracy of recognition. in the next step, data is randomly occluded and used as test dataset. precision, recall, f1-score and support have been calculated along with confusion matrix as a measure of accuracy of prediction. 

\subsection{k Nearest Neighbour}
\label{subsec:3.2 k Nearest Neighbour}
k Nearest Neighbour method is preferred over other classifiers due its simplicity and robustness in multi model classification. It also requires lesser execution time and offers better accuracy. It is efficient in classifying test data based on closest training data in the feature space. An image is classified by a taking a majority vote of the neighbouring classes. The euclidean distance between a given test and each training image features is computed. 

\begin{figure}[h!]
%\vspace{-0.2cm}
 \centering
 \includegraphics[width = 8cm]{euclidean_distance11.JPG}
 \caption{ Euclidean Distance (Source:\cite{weinberger2006distance})}
 \label{fig 1: Euclidean Distance}
\end{figure}

Where n is the number of variables, $x_i$ and $y_i$ are the values of the ith variable at points p and q respectively. 

This forms a distance matrix whose summation value is calculated and sorted in an increasing order. According to the value of k that is specified, the classes are selected and the majority class value determines the classification of the test image \cite{kaur2012k}. KNN algorithm classifies the test images by computing the most common class from the K closest training data points. Each training data point from the k closest classes casts a vote and the data points is classified as the class with most votes.

\subsection{CNN}
\label{subsec:3.3 CNN}
Cnn are made of multiple layers of neurons which can be assigned weights and biases. Input is given to each neuron which performs a dot product. neural networks are preferred for face recognition methods because of their non linear approach. They are similar to ordinary neural networks in that, they have a loss function on the final layer and use the same mechanism to train the system. Image pixels are given as an input to the network and class scores are obtained as an output. 

\begin{figure}[h!]
%\vspace{-0.2cm}
 \centering
 \includegraphics[width = 9cm]{cnn_arch.JPG}
 \caption{ Convolutional Neural Network (Source:\cite{cheung2012convolutional})}
 \label{fig 1: Convolutional Neural Network}
\end{figure}

Each layer of neurons take in a multidimensional array of numbers, performs dot product of the weights and corresponding area of the input. The output is another multidimensional array of numbers which serves as an input to the next layer. Every layer of convolutional neural network converts one activation function to another through a differentiable function. As shown in the image, there are three layers used to build a convolutional neural network: Convolution (CONV) Layer, Pooling Layer, and Fully-Connected Layer.

Input layer holds the raw image. The convolution layer computes the dot product. RELU layer is responsible for the activation function. Downsampling operation is performed by the POOL layer. Fully connected layer is responsible for providing the output with the class scores. Keeping with its name, the neurons in this layer are connected to all neurons in the previous layer. The convolution operation is conducted as follows:

\begin{figure}[h!]
%\vspace{-0.2cm}
 \centering
 \includegraphics[width = 9cm]{cnn_formula.JPG}
 \caption{ Convolutional neural network cost function(Source:\cite{lee2009convolutional})}
 \label{fig 1: Convolutional Neural Network cost function}
\end{figure}

where $h^{(l)}_j$ denotes the j-th feature map in the l-th layer, and nl is the number of feature maps in the l-th layer. The network is parameterized by weights $w_{ij}^{(l)}$ and biases $b_j^{(l)}$to be learned\cite{lee2009convolutional}.

Additionally, convolutional layer and fully connected layers perform computations not just using the activation functions from the previous layers but also using their individual weights and biases. But the other layers namely, RELU and POOL, execute a  fixed function. The parameters in the convolutional/fully connected layers are trained using the principle of gradient descent. It is a guided search method. To monitor the error in a model, a separate cost function is used. The aim is to minimise this function to achieve lowest error possible or the maximum accuracy. Gradient descent is a repetitive process of optimization by tweaking certain parameters to find the minimum of a function, i.e. to take small steps that are proportional to the negative of the gradient of the cost at every point. Gradient descent is employed to ensure that the class scores that are output from the network are consistent with the classes in the training image set. Using this underlying principle, CNN transforms raw image pixels to the final class scores. 

\subsection{Experimental Setup}
\label{subsec: 3b. Experimental Setup}


This section describes the set of experiments that have been performed to find and analyze the accuracies of the above mentioned face recognition algorithms. First, the performance of Principal Component Analysis, k Nearest Neighbour and Convolutional Neural Networks has been evaluated by implementing these algorithms on the LFW dataset. Subsequently, these algorithms have been evaluate by applying on randomly occluded images from the dataset to demonstrate their effectiveness.

To implement face recognition using PCA, the LFW image data set is loaded as numpy arrays. The data is split into training data and test data using stratified k-fold cross validation. Training data is used to train the system while the test set is used to evaluate the performance of the system. Stratification ensures that the test data covers all classes of the training data. In other words, stratification ensures that the test data is a good representation of the training data. It offers a better split in terms of bias and variance than simple cross-validation. PCA function is used to reduce the number of features and extract the eigenfaces. The most significant eigenfaces have been plotted. SVM have been used for the classification of the images. SVM marks each data point in an n-dimensional spaces according to the number of features that are selected. SVM gives an optimal hyperplane with maximum margin between classes. Data is categorized based on this hyperplane.

\begin{figure}[h!]
%\vspace{-0.2cm}
 \centering
 \includegraphics[width = 9cm]{PCA_pred.JPG}
 \caption{ Principal Component Analysis Prediction}
 \label{fig 7: Principal Component Analysis Prediction}
\end{figure}

Even though the system is not very complex, it produces efficient results. The algorithm accurately recognises a large number of images from the data. To quantify the efficiency of the algorithm, various measures from sklearn.metrics have been used. The classification report gives the precision, recall, f1-score and support for the classification. 

k-nearest neighbour algorithm goes through the entire data for every test datapoint to find the k nearest or most similar instances to the training data. Euclidean distance measure is most commonly used for this algorithm. The other widely used measures are Manhattan distance, Jaccard distance, Minkowski distance etc. scikit-learn library and functions have been used to implement k-nearest neighbour algorithm. The underlying data structure to find the nearest neighbours using Euclidean distance BallTree. \cite{ageitgey_2018}

CNN: FaceNet is a simple network that maps the image data to an Euclidean space. This uses the concept of distance and face similarity i.e. the images of same person with small distances between them will be grouped together. Further, there will be a significant difference between images of different people. Subsequently, the recognition task can be completed using knn algorithm. The system uses a triplet image set for training. The triplet is chosen such that two of the images match and one does not. Therefore, the images that match will have smaller distance and can be grouped together while the images that do not match have a larger distance between them. \footnote{https://arxiv.org/pdf/1503.03832.pdf} The model represents each the faces in the training set into a 128 dimensional vector which are labelled and used for recognition of faces. When a new test image is entered into the system, it goes through the network to form a 128 dimensional vector. This vector is then compared with the training set. The lowest distance between the test image and training set is used to recognise the face in the image. \footnote{https://github.com/davidsandberg/facenet}
After the implementation of these algorithms, the next step was to run each of the algorithms on a randomly occluded test dataset to calculate the accuracy of face recognition. 

\begin{table}[htbp]
\caption{Source/Destination Machine Combination}
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Source}&\textbf{Destination} \\
\cline{1-2} 
\hline
Host & Host  \\
\hline
Container & Container  \\
\hline
Host & Container \\
\hline
Container & Host \\
\hline
\end{tabular}
\label{table 1}
\end{center}
\end{table}


\section{Conclusion and Future Work}
\label{sec: 4.Conclusion and Future Work}

In this work, we have presented a comparative analysis of three widely used face recognition methods: Principal Component Analysis, k-Nearest Neighbour and Convolutional Neural Networks. Among these, convolutional neural networks have shown to be most accurate. Despite their high accuracy rates, CNNs have certain drawbacks. The time required to train these networks increases with the amount of data and number of layers. It also requires high computation power, processing speed and memory. As shown, each algorithm has its own benefits and drawbacks. Hence it is quite impossible to meaningfully rank these methods as there are a number of factors need to be considered. In the end it depends on the application that the appropriate algorithm can be used. There has been a lot of research on face recognition systems under controlled conditions. However there has not been enough research on face recognition under uncontrolled conditions.

For future work, the level and area of occlusion of the images can be varied to find a threshold of occlusion that can be applied to images beyond which the above described face recognition methods resort to only a chance level accuracy. In such cases, new algorithms need to be developed which can easily identify occluded images. Most of the work deals with face recognition using two dimensional images. The accuracy of prediction can be enhanced by using three dimensional face recognition systems. \cite{mileo2008wireless} 


\section*{Acknowledgment}
This work was supported by Dublin City University (DCU), Dublin, Ireland and Dr.Alessandra Mileo, School of Computing, DCU.


\bibliographystyle{IEEEtran}
\bibliography{refs.bib}

\end{document}
